{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Building AI Applications with Python & Gradio\n",
    "## Building an AI Assistant's Toolkit with MCP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: PROJECT OVERVIEW - MAKING TOOLS FOR MODELS\n",
    "\n",
    "\n",
    "Welcome back! We've built chatbots, tutors, image processors, and even multi-agent systems. Now, let's explore how to make these creations available as **standardized tools** that other AI models or applications can easily use.\n",
    "\n",
    "**The Problem:** How does an AI model (like a large language model) reliably use external functions or APIs? Traditionally, this required custom integrations (\"plugins\") for each specific tool.\n",
    "\n",
    "**The Solution: Model-Context-Protocol (MCP)**\n",
    "MCP is an open specification designed to be a **standard way for models to discover and interact with tools**. Imagine a universal plug adapter for AI tools! A model can ask an MCP-enabled service:\n",
    "1.  \"What tools do you offer?\" (by fetching a standard description called a **manifest**)\n",
    "2.  \"Okay, please use tool X with these inputs.\" (by calling a standard **action** endpoint)\n",
    "\n",
    "**Gradio makes it incredibly easy to expose *your* Python functions as MCP tools.**\n",
    "\n",
    "**In this module, we will:**\n",
    "\n",
    "1.  **Understand MCP:** Learn the core concepts of the Model-Context-Protocol.\n",
    "2.  **Expose Existing Work:** Turn our \"Advanced AI Tutor\" into an MCP tool using Gradio (`mcp=True`).\n",
    "3.  **Create a New Tool:** Build and expose a new \"Image Style Analyzer\" tool via MCP using Gradio.\n",
    "4.  **Run MCP Servers:** Learn how to run these Gradio apps so they serve MCP endpoints.\n",
    "5.  **Act as an MCP Client:** Write Python code in *this notebook* to:\n",
    "    *   Discover the capabilities (manifest) of our running tools.\n",
    "    *   Execute the tools (call actions) remotely via HTTP requests.\n",
    "6.  **Simulate Tool Use:** Create a simple \"AI Assistant\" simulation in the notebook that decides when to call these external MCP tools.\n",
    "\n",
    "**Final Goal:** Understand MCP by building two Gradio MCP servers (AI Tutor, Image Style Analyzer) and interacting with them programmatically from a Jupyter notebook, simulating how an AI could leverage these external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: UNDERSTANDING MCP (MODEL-CONTEXT-PROTOCOL)\n",
    "\n",
    "\n",
    "Before we code, let's clarify MCP. It standardizes how a **Model** (like an LLM) interacts with **Context** (external tools, APIs, functions).\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1.  **MCP Server:** An application (like our Gradio app) that exposes one or more tools via MCP. It listens for requests on specific HTTP endpoints.\n",
    "2.  **Manifest (`/mcp/manifest.json`):** A standard JSON file served by the MCP server. It describes the available tools (called \"actions\" in MCP terms):\n",
    "    *   What each tool is called (`name`).\n",
    "    *   What it does (`description`).\n",
    "    *   What inputs it needs (`parameters`).\n",
    "    *   What output it provides.\n",
    "    *   Models use this manifest to **discover** tools and understand how to use them.\n",
    "3.  **Action Endpoint (`/mcp/action`):** A standard endpoint on the MCP server where the model sends a request (usually a POST request) to *execute* a specific tool/action. The request includes the action name and the required parameters.\n",
    "\n",
    "**Why MCP?**\n",
    "\n",
    "*   **Standardization:** Models don't need custom code for every tool. If a tool speaks MCP, the model knows how to talk to it.\n",
    "*   **Discoverability:** Models can dynamically find out what tools are available in their environment.\n",
    "*   **Decoupling:** Tools can be developed and updated independently from the models that use them.\n",
    "*   **Gradio Integration:** Gradio makes *serving* MCP incredibly simple ‚Äì often just one parameter (`mcp=True`)!\n",
    "\n",
    "In this project, our Gradio apps will be the **MCP Servers**, and this notebook will act as the **MCP Client** (simulating a model wanting to use the tools).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3: SETTING UP - INSTALL LIBRARIES & API KEYS\n",
    "\n",
    "\n",
    "We'll need `gradio` to build the servers and `requests` (or `httpx`) in this notebook to act as the client. We also need `openai` and `python-dotenv` as our tools will use the OpenAI API.\n",
    "\n",
    "Ensure your `.env` file has your OpenAI API key:\n",
    "```dotenv\n",
    "OPENAI_API_KEY=sk-YourSecretOpenAIKeyGoesHereXXXXXXXXXXXXX\n",
    "```\n",
    "*(Note: For this project, we only need the OpenAI key as both tools will use it).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio openai python-dotenv requests httpx pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for the notebook client\n",
    "import os\n",
    "import requests  # For making HTTP requests\n",
    "import httpx  # An alternative async-friendly HTTP client (good practice)\n",
    "import json  # For handling JSON data (manifests, action responses)\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Image  # To display results nicely\n",
    "from openai import OpenAI  # or litellm, groq, etc.\n",
    "from agents import Agent, Runner\n",
    "from agents.mcp import MCPServerSse\n",
    "from PIL import Image\n",
    "import asyncio, pathlib\n",
    "\n",
    "# Load environment variables (needed for the Gradio apps when they run)\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# --- Helper function to display markdown nicely ---\n",
    "def print_md(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URLs where our Gradio MCP servers WILL BE running\n",
    "# IMPORTANT: Make sure these match the ports you use when running the server scripts!\n",
    "# We'll use different ports for each server.\n",
    "MCP_BASE = \"http://localhost:7860/gradio_api/mcp/sse\"  # text tutor demo you built\n",
    "\n",
    "\n",
    "mcp_tool = MCPServerSse({\n",
    "    \"name\": \"AI Tutor and Image Analyzer\",\n",
    "    \"url\": MCP_BASE,\n",
    "    \"timeout\": 30,\n",
    "    \"client_session_timeout_seconds\":60\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4: BUILDING MCP SERVER \n",
    "\n",
    "\n",
    "Let's take our Advanced AI Tutor code and also lets add image analyzer tool and expose its core function as an MCP tool.\n",
    "\n",
    "**The Key Change:** Adding `.launch(mcp=True)`\n",
    "\n",
    "**Instructions:**\n",
    "1. Open the `tutor_mcp_server.ipynb` and run it\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Opportunity:\n",
    "\n",
    "**Verify Server is Running:** Open your web browser to `http://localhost:7860/gradio_api/mcp/sse`. You should see the familiar AI Tutor Gradio interface. The crucial part is that it's *also* serving the MCP endpoints in the background.\n",
    "\n",
    "What did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5: DISCOVERING TOOLS - FETCHING THE MANIFEST\n",
    "\n",
    "Our MCP servers are running. Now, from this notebook, let's act like a client (or an LLM) wanting to see what tools they offer. We do this by fetching the `/mcp/schema` endpoint using an HTTP GET request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use httpx for modern async-friendly requests (though requests works fine too)\n",
    "client = httpx.Client()  # Create an HTTP client instance\n",
    "\n",
    "\n",
    "def fetch_schema(server_url):\n",
    "    \"\"\"Fetches and parses the MCP schema from a server.\"\"\"\n",
    "    schema_url = server_url.replace(\"/sse\", \"/schema\")\n",
    "    print(f\"Fetching schema from: {schema_url}\")\n",
    "    response = client.get(schema_url, timeout=10)  # Add a timeout\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    schema_data = response.json()\n",
    "    print(\"Schema fetched successfully!\")\n",
    "    return schema_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fetching AI Tutor Manifest ---\n",
      "Fetching schema from: http://localhost:7860/gradio_api/mcp/schema\n",
      "Schema fetched successfully!\n",
      "\n",
      "AI Tutor Manifest Contents:\n",
      "{\n",
      "  \"explain_concept\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"question\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"level\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"numeric value between 1 and 5\"\n",
      "      }\n",
      "    },\n",
      "    \"description\": \"Stream an explanation of *question* at the requested *level* (1\\u20115).\"\n",
      "  },\n",
      "  \"summarize_text\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"text\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"compression_ratio\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"numeric value between 0.1 and 0.8\"\n",
      "      }\n",
      "    },\n",
      "    \"description\": \"Stream a summary of *text* compressed to roughly *compression_ratio* length. *compression_ratio* should be between 0.1 and 0.8.\"\n",
      "  },\n",
      "  \"generate_flashcards\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"topic\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"num_cards\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"numeric value between 1 and 20\"\n",
      "      }\n",
      "    },\n",
      "    \"description\": \"Stream *num_cards* Q/A flashcards for *topic* in JSON lines format.\"\n",
      "  },\n",
      "  \"quiz_me\": {\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "      \"topic\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"level\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"numeric value between 1 and 5\"\n",
      "      },\n",
      "      \"num_questions\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"numeric value between 1 and 15\"\n",
      "      }\n",
      "    },\n",
      "    \"description\": \"Stream a quiz with numbered Qs then reveal answers after all questions.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch manifest from the AI Tutor server\n",
    "print(\"--- Fetching AI Tutor Schema ---\")\n",
    "tutor_schema = fetch_schema(MCP_BASE)\n",
    "\n",
    "if tutor_schema:\n",
    "    print(\"\\nAI Tutor Schema Contents:\")\n",
    "    # Pretty print the JSON manifest\n",
    "    print(json.dumps(tutor_schema, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")  # Separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRACTICE OPPORTUNITY:\n",
    "\n",
    "1.  Stop the Gradio servers.\n",
    "2.  Edit its notebook file (`tutor_mcp_server.ipynb`).\n",
    "3.  Improve the **docstring** of the core function (`explain_concept`) to be more descriptive for an LLM. For example, add details about in what level of detail the tool will explain the concept.\n",
    "4.  Restart the Gradio server.\n",
    "5.  Re-run the notebook cell above that fetches the schema.\n",
    "6.  Has it updated with your improved text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 7: Create an agent that uses the tools\n",
    "\n",
    "In this step, we'll put the discovered tools into action by creating an agent that can interact with them.\n",
    "\n",
    "Add the mcp_servers to the agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 3)  Build the agent\n",
    "# ----------------------------------------------------------------------\n",
    "agent = Agent(\n",
    "    name=\"Smart‚ÄØAssistant\",\n",
    "    instructions=\"\"\"\n",
    "    Context\n",
    "    -------\n",
    "    You are an AI assistant with access to an MCP server exposing **four streaming tools**:\n",
    "\n",
    "    1. **explain_concept**  \n",
    "    Arguments: { \"question\": <str>, \"level\": <int 1‚Äë5> }  \n",
    "    ‚Ä¢ Streams an explanation of any concept at the requested depth.\n",
    "\n",
    "    2. **summarize_text**  \n",
    "    Arguments: { \"text\": <str>, \"compression_ratio\": <float 0.1‚Äë0.8> }  \n",
    "    ‚Ä¢ Streams a concise summary ~compression_ratio √ó original length.\n",
    "\n",
    "    3. **generate_flashcards**  \n",
    "    Arguments: { \"topic\": <str>, \"num_cards\": <int 1‚Äë20> }  \n",
    "    ‚Ä¢ Streams JSON‚Äëlines flashcards: one card per line `{ \"q\":‚Ä¶, \"a\":‚Ä¶ }`.\n",
    "\n",
    "    4. **quiz_me**  \n",
    "    Arguments: { \"topic\": <str>, \"level\": <int 1‚Äë5>, \"num_questions\": <int 1‚Äë15> }  \n",
    "    ‚Ä¢ Streams an MC‚Äëquestion quiz, then an ANSWER‚ÄØKEY section.\n",
    "\n",
    "    Objective\n",
    "    ---------\n",
    "    Help users learn by:\n",
    "    ‚Ä¢ Explaining concepts at the depth they request.  \n",
    "    ‚Ä¢ Summarising long passages.  \n",
    "    ‚Ä¢ Generating flashcards for self‚Äëstudy.  \n",
    "    ‚Ä¢ Quizzing them interactively.\n",
    "\n",
    "    How to respond\n",
    "    --------------\n",
    "    ‚Ä¢ For each user request, decide which tool (if any) fulfils it best.  \n",
    "    ‚Ä¢ Call the tool via MCP by returning *only* the JSON with `\"tool\"` and `\"arguments\"` (no extra text).  \n",
    "    ‚Ä¢ If a follow‚Äëup conversation is needed (e.g., clarification), ask the user first.  \n",
    "    ‚Ä¢ If no tool fits, answer directly in plain language.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    User: ‚ÄúExplain quantum tunnelling like I‚Äôm 10.‚Äù  \n",
    "    ‚Üí Call `explain_concept` with { \"question\": \"quantum tunnelling\", \"level\": 2 }\n",
    "\n",
    "    User: ‚ÄúSummarise this article to 20‚ÄØ%.‚Äù + <article text>  \n",
    "    ‚Üí Call `summarize_text` with { \"text\": \"...\", \"compression_ratio\": 0.2 }\n",
    "\n",
    "    Chat capability\n",
    "    ---------------\n",
    "    After each tool call completes (streaming back to the user), remain in the chat loop ready for the next user turn.\n",
    "    \"\"\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    mcp_servers=[mcp_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "can you expalin llm to a 5 year old"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A large language model (LLM) is like a super-smart robot that loves to read. It learns from tons of books and stories. After reading a lot, it can answer questions and help you talk about different things, just like how you learn new words and ideas!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "can you create 5 flash cards on this topic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are 5 flashcards on the topic of large language models (LLMs):\n",
       "\n",
       "1. **Q:** What does LLM stand for?  \n",
       "   **A:** Large Language Model\n",
       "\n",
       "2. **Q:** What do LLMs learn from?  \n",
       "   **A:** They learn from a lot of text, like books and articles.\n",
       "\n",
       "3. **Q:** What can LLMs do?  \n",
       "   **A:** They can answer questions and help you talk about different subjects.\n",
       "\n",
       "4. **Q:** How do LLMs become smart?  \n",
       "   **A:** By reading and understanding a huge amount of information.\n",
       "\n",
       "5. **Q:** Can LLMs write stories?  \n",
       "   **A:** Yes, they can create stories and text based on what they've learned!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "i woould like to have a quiz on this"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here‚Äôs a quiz on large language models (LLMs):\n",
       "\n",
       "### Quiz Questions:\n",
       "\n",
       "1. **What does LLM stand for?**\n",
       "   - A) Little Language Model\n",
       "   - B) Large Language Model\n",
       "   - C) Long Language Model\n",
       "\n",
       "2. **What do LLMs read to learn?**\n",
       "   - A) Pictures\n",
       "   - B) Lots of text like books and articles\n",
       "   - C) Only newspapers\n",
       "\n",
       "3. **What can LLMs help you with?**\n",
       "   - A) Cooking\n",
       "   - B) Answering questions and talking about subjects\n",
       "   - C) Math problems only\n",
       "\n",
       "4. **How do LLMs become smart?**\n",
       "   - A) By watching TV\n",
       "   - B) By reading a huge amount of information\n",
       "   - C) By listening to music\n",
       "\n",
       "5. **Can LLMs create stories?**\n",
       "   - A) Yes\n",
       "   - B) No\n",
       "   - C) Only if they are short\n",
       "\n",
       "---\n",
       "\n",
       "### Answer Key:\n",
       "1. B) Large Language Model\n",
       "2. B) Lots of text like books and articles\n",
       "3. B) Answering questions and talking about subjects\n",
       "4. B) By reading a huge amount of information\n",
       "5. A) Yes\n",
       "\n",
       "Feel free to let me know your answers!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "b, b,b,b,a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's check your answers!\n",
       "\n",
       "1. **What does LLM stand for?**  \n",
       "   **Your answer:** B) Large Language Model  ‚úîÔ∏è  \n",
       "   \n",
       "2. **What do LLMs read to learn?**  \n",
       "   **Your answer:** B) Lots of text like books and articles  ‚úîÔ∏è  \n",
       "\n",
       "3. **What can LLMs help you with?**  \n",
       "   **Your answer:** B) Answering questions and talking about subjects  ‚úîÔ∏è  \n",
       "\n",
       "4. **How do LLMs become smart?**  \n",
       "   **Your answer:** B) By reading a huge amount of information  ‚úîÔ∏è  \n",
       "\n",
       "5. **Can LLMs create stories?**  \n",
       "   **Your answer:** A) Yes  ‚úîÔ∏è  \n",
       "\n",
       "**Great job!** You answered all questions correctly! üéâ If you want to learn more or take another quiz, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "can you summerize what ever we talked"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a summary of our conversation:\n",
       "\n",
       "We talked about large language models (LLMs), describing them as smart robots that learn by reading a lot and can help answer questions. We created five flashcards highlighting key aspects of LLMs, such as what they are, how they learn, and what they can do. You also took a quiz on LLMs and answered all questions correctly! \n",
       "\n",
       "If you need anything else, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await mcp_tool.connect()  # open SSE channels\n",
    "\n",
    "result = None\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "    if result is not None:\n",
    "        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    else:\n",
    "        new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    print(\"\\nUser Input:\")\n",
    "    print_md(user_input)\n",
    "    result = await Runner.run(starting_agent=agent, input=new_input)\n",
    "    print(\"\\nAssistant:\")\n",
    "    print_md(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool:  explain_concept\n",
      "Arguments:  {\"question\":\"what is a large language model (LLM)\",\"level\":1}\n",
      "Tool:  generate_flashcards\n",
      "Arguments:  {\"topic\":\"large language model (LLM)\",\"num_cards\":5}\n",
      "Tool:  quiz_me\n",
      "Arguments:  {\"topic\":\"large language model (LLM)\",\"level\":1,\"num_questions\":5}\n",
      "Tool:  summarize_text\n",
      "Arguments:  {\"text\":\"We discussed large language models (LLMs), explaining them simply, as smart robots that read a lot to learn and help answer questions. We created five flashcards covering key concepts about LLMs, such as their definition, learning methods, and capabilities, like answering questions and writing stories. Finally, you took a quiz on LLMs and answered all questions correctly.\",\"compression_ratio\":0.3}\n"
     ]
    }
   ],
   "source": [
    "for i in result.to_input_list():\n",
    "    for key in i.keys():\n",
    "        if key == 'arguments':\n",
    "            print(\"Tool: \", i['name'])\n",
    "            print(\"Arguments: \", i['arguments'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRACTICE OPPORTUNITY: \n",
    "\n",
    "1.  Try to call it without the if-else statement.\n",
    "\n",
    "```python\n",
    "    if result is not None:\n",
    "        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    else:\n",
    "        new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "```\n",
    "\n",
    "use only \n",
    "\n",
    "```python\n",
    "    new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "```\n",
    "\n",
    "What did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Opportunity Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Opportunity 1 Solution:\n",
    "\n",
    "**Verify Server is Running:** Open your web browser to `http://localhost:7860/gradio_api/mcp/sse`. You should see the familiar AI Tutor Gradio interface. The crucial part is that it's *also* serving the MCP endpoints in the background.\n",
    "\n",
    "What did you notice?\n",
    "\n",
    "-------\n",
    "\n",
    "If active, you will see some text like this:\n",
    "\n",
    "```python\n",
    "event: endpoint\n",
    "data: /gradio_api/mcp/messages/?session_id=f726c2ebd68644a38d15b44eec76ece3\n",
    "\n",
    ": ping - 2025-05-06 19:38:16.337930+00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Opportunity 2 Solution:\n",
    "\n",
    "1.  Stop the Gradio servers.\n",
    "2.  Edit its notebook file (`tutor_mcp_server.ipynb`).\n",
    "3.  Improve the **docstring** of the core function (`explain_concept`) to be more descriptive for an LLM. For example, add details about in what level of detail the tool will explain the concept.\n",
    "4.  Restart the Gradio server.\n",
    "5.  Re-run the notebook cell above that fetches the schema.\n",
    "6.  Has it updated with your improved text?\n",
    "\n",
    "-------\n",
    "\n",
    "```python\n",
    "# in the tutor_mcp_server.ipynb file\n",
    "\n",
    "# Update the doc string in the tutor_mcp_server.ipynb file\n",
    "\n",
    "\n",
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1‚Äë5). If 1, explanation would be like we are talking to a 5 year old and if 5, explanation would be technical and complex.\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Opportunity 3 Solution:\n",
    "\n",
    "1.  Try to call it without the if-else statement.\n",
    "\n",
    "```python\n",
    "    if result is not None:\n",
    "        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    else:\n",
    "        new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "```\n",
    "\n",
    "use only \n",
    "\n",
    "```python\n",
    "    new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "```\n",
    "\n",
    "What did you notice?\n",
    "\n",
    "---------\n",
    "\n",
    "- You will probably see for each call, models doesn't know what we talked before. This is because we are not using the context from the previous calls. `result.to_input_list()` is used to get the context from the previous calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
